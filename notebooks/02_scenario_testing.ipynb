{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0 Simulation - Stage 2: Full Scenario Testing\n",
    "\n",
    "Runs all test cases T001-T010 (Scenarios A-G) with full integration:\n",
    "- Mana distribution (75% deterministic + 25% Bloom VRF)\n",
    "- Community detection (monthly Louvain clustering)\n",
    "- Decay engine (progressive + anomaly-accelerated)\n",
    "- Grace Period for returning users\n",
    "\n",
    "**Success Criteria:**\n",
    "- Sybil ROI(k) < 1/k\n",
    "- TPR > 95%, FPR < 1%\n",
    "- Gini: 0.2 - 0.3\n",
    "- Community stability > 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/kunimitsu/Projects/Meguri_pre3')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from simulation.runner import SimulationRunner\n",
    "from simulation.config import MeguriPhase0Config, TestCase\n",
    "from simulation.metrics import generate_summary_report\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "config = MeguriPhase0Config()\n",
    "runner = SimulationRunner()\n",
    "\n",
    "print(f\"Loaded {len(config.test_cases)} test cases\")\n",
    "print(\"Ready to run Stage 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run All Test Cases (T001-T009)\n",
    "\n",
    "T004 (N=100K) and T010 (variable k) are skipped in this quick run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters (will be optimized in Stage 3)\n",
    "KAPPA = 1.0\n",
    "THETA = 0.3\n",
    "BETA = 1.0\n",
    "BLOOM_INTERVAL = 7\n",
    "\n",
    "start = time.time()\n",
    "all_results = runner.run_all_test_cases(\n",
    "    kappa=KAPPA, theta=THETA, beta=BETA,\n",
    "    bloom_interval=BLOOM_INTERVAL, verbose=True\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal elapsed time: {elapsed:.1f}s\")\n",
    "print(f\"Completed {len(all_results)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary DataFrame\n",
    "summary_rows = []\n",
    "for r in all_results:\n",
    "    summary_rows.append({\n",
    "        'Test ID': r.get('test_id', ''),\n",
    "        'Scenario': r.get('scenario', ''),\n",
    "        'N': r.get('network_size', ''),\n",
    "        'k': r.get('attacker_count', 0),\n",
    "        'Sybil ROI': r.get('sybil_roi', None),\n",
    "        'ROI Threshold': r.get('sybil_roi_threshold', None),\n",
    "        'ROI Pass': r.get('sybil_roi_pass', None),\n",
    "        'TPR': r.get('tpr', None),\n",
    "        'FPR': r.get('fpr', None),\n",
    "        'TPR Pass': r.get('tpr_pass', None),\n",
    "        'FPR Pass': r.get('fpr_pass', None),\n",
    "        'Gini': r.get('gini', None),\n",
    "        'Bloom Events': r.get('bloom_events', 0),\n",
    "        'Stabilization (days)': r.get('stabilization_median_days', None),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sybil ROI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter scenario A tests (T001-T003)\n",
    "scenario_a = df_summary[df_summary['Scenario'] == 'A'].copy()\n",
    "\n",
    "if not scenario_a.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Sybil ROI vs k\n",
    "    ax1 = axes[0]\n",
    "    k_vals = scenario_a['k'].values\n",
    "    roi_vals = scenario_a['Sybil ROI'].values\n",
    "    threshold_vals = scenario_a['ROI Threshold'].values\n",
    "    \n",
    "    ax1.plot(k_vals, roi_vals, 'o-', color='#ff6b6b', markersize=8, label='Actual ROI')\n",
    "    ax1.plot(k_vals, threshold_vals, 's--', color='#51cf66', markersize=6, label='Target (1/k)')\n",
    "    ax1.set_xlabel('Number of Attacker Nests (k)')\n",
    "    ax1.set_ylabel('Sybil ROI')\n",
    "    ax1.set_title('Sybil ROI vs Attacker Count (Scenario A)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # TPR and FPR for all scenarios\n",
    "    ax2 = axes[1]\n",
    "    test_ids = df_summary['Test ID'].values\n",
    "    tpr_vals = df_summary['TPR'].values * 100\n",
    "    fpr_vals = df_summary['FPR'].values * 100\n",
    "    \n",
    "    x = np.arange(len(test_ids))\n",
    "    width = 0.35\n",
    "    ax2.bar(x - width/2, tpr_vals, width, label='TPR (%)', color='#4ecdc4')\n",
    "    ax2.bar(x + width/2, fpr_vals, width, label='FPR (%)', color='#ff6b6b')\n",
    "    ax2.axhline(y=95, color='#4ecdc4', linestyle='--', alpha=0.5, label='TPR target (95%)')\n",
    "    ax2.axhline(y=1, color='#ff6b6b', linestyle='--', alpha=0.5, label='FPR target (1%)')\n",
    "    ax2.set_xlabel('Test Case')\n",
    "    ax2.set_ylabel('Rate (%)')\n",
    "    ax2.set_title('Detection Rates by Test Case')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(test_ids, rotation=45)\n",
    "    ax2.legend(fontsize=8)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/Users/kunimitsu/Projects/Meguri_pre3/results/stage2_sybil_analysis.png',\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No Scenario A results to plot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Balance Distribution & Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Gini coefficient by test case\n",
    "ax1 = axes[0]\n",
    "gini_vals = df_summary['Gini'].values\n",
    "colors = ['#51cf66' if 0.2 <= g <= 0.3 else ('#ffec99' if g <= 0.5 else '#ff6b6b')\n",
    "          for g in gini_vals]\n",
    "ax1.bar(df_summary['Test ID'], gini_vals, color=colors)\n",
    "ax1.axhline(y=0.2, color='green', linestyle='--', alpha=0.5, label='Target lower (0.2)')\n",
    "ax1.axhline(y=0.3, color='green', linestyle='--', alpha=0.5, label='Target upper (0.3)')\n",
    "ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Warning (0.5)')\n",
    "ax1.set_xlabel('Test Case')\n",
    "ax1.set_ylabel('Gini Coefficient')\n",
    "ax1.set_title('Balance Distribution Fairness')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Bloom event distribution\n",
    "ax2 = axes[1]\n",
    "bloom_vals = df_summary['Bloom Events'].values\n",
    "ax2.bar(df_summary['Test ID'], bloom_vals, color='#74b9ff')\n",
    "ax2.set_xlabel('Test Case')\n",
    "ax2.set_ylabel('Bloom Events')\n",
    "ax2.set_title('Bloom Event Count by Test Case')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/kunimitsu/Projects/Meguri_pre3/results/stage2_fairness.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Scenario Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detailed scores for a specific test case\n",
    "def analyze_test_case(test_id):\n",
    "    \"\"\"Load and analyze detailed scores for a test case.\"\"\"\n",
    "    csv_path = Path(f'/Users/kunimitsu/Projects/Meguri_pre3/results/scores/{test_id}_scores.csv')\n",
    "    if not csv_path.exists():\n",
    "        print(f'No score data for {test_id}')\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Final day analysis\n",
    "    final_day = df['day'].max()\n",
    "    final = df[df['day'] == final_day]\n",
    "    honest = final[final['is_attacker'] == 0]\n",
    "    attackers = final[final['is_attacker'] == 1]\n",
    "    \n",
    "    print(f'\\n--- {test_id} (Day {final_day}) ---')\n",
    "    print(f'Honest:    N={len(honest)}, S_mean={honest[\"S\"].mean():.4f}, '\n",
    "          f'F_rate={honest[\"F\"].mean():.2%}, balance={honest[\"balance\"].mean():.2f}')\n",
    "    if len(attackers) > 0:\n",
    "        print(f'Attackers: N={len(attackers)}, S_mean={attackers[\"S\"].mean():.4f}, '\n",
    "              f'F_rate={attackers[\"F\"].mean():.2%}, balance={attackers[\"balance\"].mean():.2f}')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze key scenarios\n",
    "for r in all_results:\n",
    "    analyze_test_case(r['test_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score Evolution Over 180 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a medium-scale test case for evolution analysis\n",
    "target_tid = None\n",
    "for r in all_results:\n",
    "    if r.get('scenario') == 'A' and r.get('attacker_count', 0) >= 5:\n",
    "        target_tid = r['test_id']\n",
    "        break\n",
    "\n",
    "if target_tid:\n",
    "    csv_path = Path(f'/Users/kunimitsu/Projects/Meguri_pre3/results/scores/{target_tid}_scores.csv')\n",
    "    if csv_path.exists():\n",
    "        df_evo = pd.read_csv(csv_path)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(f'Score Evolution: {target_tid}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Average S(v) over time: honest vs attackers\n",
    "        honest_daily = df_evo[df_evo['is_attacker'] == 0].groupby('day')\n",
    "        attacker_daily = df_evo[df_evo['is_attacker'] == 1].groupby('day')\n",
    "        \n",
    "        axes[0,0].plot(honest_daily['S'].mean(), label='Honest (mean)', color='#4ecdc4')\n",
    "        axes[0,0].fill_between(honest_daily['S'].mean().index,\n",
    "                              honest_daily['S'].mean() - honest_daily['S'].std(),\n",
    "                              honest_daily['S'].mean() + honest_daily['S'].std(),\n",
    "                              alpha=0.2, color='#4ecdc4')\n",
    "        if len(attacker_daily) > 0:\n",
    "            axes[0,0].plot(attacker_daily['S'].mean(), label='Attackers (mean)', color='#ff6b6b')\n",
    "        axes[0,0].axhline(y=THETA, color='yellow', linestyle='--', alpha=0.5, label=f'θ={THETA}')\n",
    "        axes[0,0].set_title('Integrated Score S(v)')\n",
    "        axes[0,0].legend(fontsize=8)\n",
    "        axes[0,0].grid(alpha=0.3)\n",
    "        \n",
    "        # Average balance over time\n",
    "        axes[0,1].plot(honest_daily['balance'].mean(), label='Honest', color='#4ecdc4')\n",
    "        if len(attacker_daily) > 0:\n",
    "            axes[0,1].plot(attacker_daily['balance'].mean(), label='Attackers', color='#ff6b6b')\n",
    "        axes[0,1].set_title('Average Balance')\n",
    "        axes[0,1].legend(fontsize=8)\n",
    "        axes[0,1].grid(alpha=0.3)\n",
    "        \n",
    "        # F(v) flag rate over time\n",
    "        axes[1,0].plot(honest_daily['F'].mean() * 100, label='Honest FPR', color='#4ecdc4')\n",
    "        if len(attacker_daily) > 0:\n",
    "            axes[1,0].plot(attacker_daily['F'].mean() * 100, label='Attacker TPR', color='#ff6b6b')\n",
    "        axes[1,0].axhline(y=1, color='yellow', linestyle='--', alpha=0.5, label='FPR target (1%)')\n",
    "        axes[1,0].set_title('Anomaly Flag Rate (%)')\n",
    "        axes[1,0].legend(fontsize=8)\n",
    "        axes[1,0].grid(alpha=0.3)\n",
    "        \n",
    "        # Decay acceleration g(S) over time\n",
    "        axes[1,1].plot(honest_daily['g_S'].mean(), label='Honest', color='#4ecdc4')\n",
    "        if len(attacker_daily) > 0:\n",
    "            axes[1,1].plot(attacker_daily['g_S'].mean(), label='Attackers', color='#ff6b6b')\n",
    "        axes[1,1].axhline(y=1.0, color='yellow', linestyle='--', alpha=0.5, label='Normal (g=1)')\n",
    "        axes[1,1].set_title('Decay Acceleration g(S)')\n",
    "        axes[1,1].legend(fontsize=8)\n",
    "        axes[1,1].grid(alpha=0.3)\n",
    "        \n",
    "        for ax in axes.flat:\n",
    "            ax.set_xlabel('Day')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/Users/kunimitsu/Projects/Meguri_pre3/results/stage2_evolution.png',\n",
    "                    dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    print('No suitable test case found for evolution analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Success Criteria Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('PHASE 0 SUCCESS CRITERIA CHECKLIST')\n",
    "print('='*60)\n",
    "\n",
    "checks = [\n",
    "    ('Sybil ROI(100) < 1/150', 'sybil_roi_pass', 'T004'),\n",
    "    ('False Positive Rate < 1%', 'fpr_pass', None),\n",
    "    ('κ, θ optimal values determined', None, 'Stage 3'),\n",
    "    ('Bloom event interval determined', None, 'Stage 3'),\n",
    "    ('Analysis cycle determined', None, 'Stage 3'),\n",
    "    ('Grace Period determined', None, 'Stage 3'),\n",
    "    ('Community detection algorithm selected', None, 'Louvain'),\n",
    "    ('Distance axis necessity concluded', None, 'T010'),\n",
    "]\n",
    "\n",
    "for check_name, metric_key, note in checks:\n",
    "    if metric_key:\n",
    "        # Check across all results\n",
    "        passes = all(r.get(metric_key, False) for r in all_results if r.get('attacker_count', 0) > 0)\n",
    "        status = '☑' if passes else '☐'\n",
    "    else:\n",
    "        status = '☐'\n",
    "    \n",
    "    note_str = f' ({note})' if note else ''\n",
    "    print(f'  {status} {check_name}{note_str}')\n",
    "\n",
    "print()\n",
    "print('Legend: ☑ = Achieved, ☐ = Pending (Stage 3 optimization needed)')\n",
    "print('\\nNext: Run notebooks/03_parameter_optimization.ipynb for DoE optimization')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
